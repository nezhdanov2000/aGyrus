#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è GPT-2 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ transformers
"""

import sys
import io

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def generate_text(prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.9):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - prompt: –Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å—Ç—Ä–æ–∫–∞)
    - max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    - temperature: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å (–º–µ–Ω—å—à–µ = –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ)
    - top_k: –≤—ã–±–∏—Ä–∞–µ—Ç –∏–∑ top-k –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö —Å–ª–æ–≤
    - top_p: nucleus sampling (—Å—É–º–º–∞—Ä–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
    """
    print("\nü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å GPT-2...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    
    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    model.eval()
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\n")
    print(f"üìù –ü—Ä–æ–º–ø—Ç: {prompt}\n")
    print("=" * 60)
    
    # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=max_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True
        )
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    print(generated_text)
    print("=" * 60)
    
    return generated_text


def interactive_mode():
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º - –º–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª–∏
    """
    print("\n" + "=" * 60)
    print("üöÄ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú GPT-2")
    print("=" * 60)
    print("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    print("–í—ã –º–æ–∂–µ—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ generate_text()\n")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑
    print("ü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å GPT-2...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model.eval()
    print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!\n")
    
    while True:
        try:
            prompt = input("\nüìù –í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º–ø—Ç >>> ")
            
            if prompt.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
                
            if not prompt.strip():
                print("‚ö†Ô∏è –ü—Ä–æ–º–ø—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
                continue
            
            print("\n‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–µ–∫—Å—Ç...")
            print("=" * 60)
            
            # –ö–æ–¥–∏—Ä—É–µ–º –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            input_ids = tokenizer.encode(prompt, return_tensors='pt')
            
            with torch.no_grad():
                output = model.generate(
                    input_ids,
                    max_length=150,
                    temperature=0.8,
                    top_k=50,
                    top_p=0.95,
                    num_return_sequences=1,
                    pad_token_id=tokenizer.eos_token_id,
                    do_sample=True
                )
            
            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
            print(generated_text)
            print("=" * 60)
            
        except KeyboardInterrupt:
            print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("üéØ –¢–ï–°–¢ GPT-2")
    print("=" * 60)
    
    # –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    print("\nüìå –ü—Ä–∏–º–µ—Ä 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞")
    generate_text(
        prompt="Once upon a time",
        max_length=100,
        temperature=0.7
    )
    
    # –ü—Ä–∏–º–µ—Ä 2: –î—Ä—É–≥–æ–π –ø—Ä–æ–º–ø—Ç
    print("\nüìå –ü—Ä–∏–º–µ—Ä 2: –î—Ä—É–≥–æ–π —Å—Ç–∏–ª—å")
    generate_text(
        prompt="The future of artificial intelligence",
        max_length=120,
        temperature=0.8
    )
    
    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\n" + "=" * 60)
    choice = input("\nüîÑ –•–æ—Ç–∏—Ç–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º? (y/n): ")
    
    if choice.lower() in ['y', 'yes', '–¥', '–¥–∞']:
        interactive_mode()
    else:
        print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
